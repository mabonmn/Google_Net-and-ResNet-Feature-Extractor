{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features_colab-P2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Z1asTDTzBai-Rfk5pRvDKZTA59IKzHak",
      "authorship_tag": "ABX9TyNltNFIiPKyUsaplWBjbq5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabonmn/Google_Net-and-ResNet-Feature-Extractor/blob/main/Features_colab_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkBilRh4U5jW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39448e7a-a9a7-40ea-fc68-9bf19ae026e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Defining the Class FeaturesEXtractor\n",
        "Methods:\n",
        "     1._initialize_transformation:\n",
        "         Define the image preprocessing. Resize, Centercrop, Convert to tensor followed by Normalization.\n",
        " \n",
        "     2._initialize_major_model:\n",
        "         Define GoogLeNet (import the model)\n",
        "         Remove the last FC layer of the model to extract feature from Pool5 layer\n",
        "         \n",
        "     3._initialize_dataset:\n",
        "         Create h5 file to store features of each video\n",
        "         \n",
        "     4._initialize_video_paths:\n",
        "         Define each videos path\n",
        "         \n",
        "     5._is_filename_valid:\n",
        "         Verify that vidoes are in the correct format '.mp4'\n",
        "         \n",
        "     6._get_video_name:\n",
        "         Store the name of video\n",
        "         \n",
        "     7._extract_features:\n",
        "         Function to extract feature from the the pool layer as determined from the model. \n",
        "         It is at this function where the image is preprocessed and the sent to the model for features extraction\n",
        "         \n",
        "     8._create_attributes:\n",
        "         Define the attributes and the format of the h5 File.\n",
        "         \n",
        "    10.perform_extraction:\n",
        "        Extract frames from vidoe and downsample to 1 frame per 15. This the calls the feature extraction function\n",
        "        Data is then\n",
        "    \n",
        "        \n",
        "'''"
      ],
      "metadata": {
        "id": "o9t1AHztZQf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c832ce1e-dc7e-4f7e-e936-11d4a3035bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nDefining the Class FeaturesEXtractor\\nMethods:\\n     1._initialize_transformation:\\n         Define the image preprocessing. Resize, Centercrop, Convert to tensor followed by Normalization.\\n \\n     2._initialize_major_model:\\n         Define GoogLeNet (import the model)\\n         Remove the last FC layer of the model to extract feature from Pool5 layer\\n         \\n     3._initialize_dataset:\\n         Create h5 file to store features of each video\\n         \\n     4._initialize_video_paths:\\n         Define each videos path\\n         \\n     5._is_filename_valid:\\n         Verify that vidoes are in the correct format '.mp4'\\n         \\n     6._get_video_name:\\n         Store the name of video\\n         \\n     7._extract_features:\\n         Function to extract feature from the the pool layer as determined from the model. \\n         It is at this function where the image is preprocessed and the sent to the model for features extraction\\n         \\n     8._create_attributes:\\n         Define the attributes and the format of the h5 File.\\n         \\n    10.perform_extraction:\\n        Extract frames from vidoe and downsample to 1 frame per 15. This the calls the feature extraction function\\n        Data is then\\n    \\n        \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeatureExtractor():\n",
        "    \n",
        "    #Initialising the class\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocess = self._initialize_transformation()\n",
        "        self.major_model = self._initialize_major_model()\n",
        "        self.minor_model = self._initialize_minor_model()\n",
        "        return\n",
        "\n",
        "\n",
        "    # initialize the transformation function for preprocessing images\n",
        "    def _initialize_transformation(self):\n",
        "        transformation = transforms.Compose([transforms.Resize(256),\n",
        "                                             transforms.CenterCrop(224),\n",
        "                                             transforms.ToTensor(),\n",
        "                                            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "        return transformation\n",
        "\n",
        "    def _initialize_major_model(self):\n",
        "        model = models.googlenet(pretrained = True)\n",
        "        model = nn.Sequential(*list(model.children())[:-1])\n",
        "        model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "        return model\n",
        "\n",
        "\n",
        "    def _initialize_minor_model(self):\n",
        "        model = models.resnet152(pretrained=True)\n",
        "        model = nn.Sequential(*list(model.children())[:-1])\n",
        "        model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "        return model\n",
        "\n",
        "\n",
        "    def _initialize_dataset(self, save_path):\n",
        "        dataset = h5py.File(save_path, 'w')\n",
        "        return dataset\n",
        "\n",
        "    def _initialize_video_paths(self, video_path):\n",
        "        if os.path.isdir(video_path):\n",
        "            video_paths = [f'{video_path}/{filename}' for filename in os.listdir(video_path) if self._is_filename_valid(filename)]\n",
        "        else:\n",
        "            video_paths = [video_path]\n",
        "        return video_paths\n",
        "\n",
        "    def _is_filename_valid(self, filename):\n",
        "        return True if '.mp4' in filename else False\n",
        "\n",
        "    def _get_video_name(self, video_path):\n",
        "        return video_path.split('/')[-1].split('.')[0]\n",
        "\n",
        "\n",
        "    # preprocess and extract the features from the image\n",
        "    def _extract_features(self, image):\n",
        "        image_tensor = self.preprocess(image)\n",
        "        image_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "        # move the input and model to GPU for speed if available\n",
        "        if torch.cuda.is_available():\n",
        "            image_batch = image_batch.to('cuda')\n",
        "            self.model.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            major_output = self.major_model(image_batch)\n",
        "            minor_output = self.minor_model(image_batch)\n",
        "        return major_output[0].cpu().view(-1).numpy(), minor_output[0].cpu().view(-1).numpy()\n",
        "\n",
        "\n",
        "    def _create_attributes(self, video_name, fps, number_of_frames, major_features,minor_features):\n",
        "        self.dataset.create_group(video_name)\n",
        "        self.dataset[video_name]['name'] = video_name\n",
        "        self.dataset[video_name]['fps'] = fps\n",
        "        self.dataset[video_name]['number_of_frames'] = number_of_frames\n",
        "        self.dataset[video_name]['number_of_downsampled_frames'] = len(major_features)\n",
        "        self.dataset[video_name]['major_features'] = major_features                           # (number_of_downsampled_frames x feature_dimension)\n",
        "        self.dataset[video_name]['minor_features'] = minor_features                           # (number_of_downsampled_frames x feature_dimension)\n",
        "\n",
        "        return\n",
        "    \n",
        "    def _get_h5py_empty(self):\n",
        "        return h5py.Empty(dtype = np.int64)\n",
        "\n",
        "\n",
        "    def perform_extraction(self, save_path, video_path):\n",
        "        self.dataset = self._initialize_dataset(save_path)\n",
        "        self.video_paths = self._initialize_video_paths(video_path)\n",
        "\n",
        "\n",
        "        for video_path in tqdm(self.video_paths):\n",
        "            major_features = []\n",
        "            minor_features = []\n",
        "            video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "            video_name = self._get_video_name(video_path)\n",
        "            fps = video_capture.get(cv2.CAP_PROP_FPS) #from opencv to find frames per sec\n",
        "            number_of_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            print(video_name)\n",
        "\n",
        "            for index in range(number_of_frames):\n",
        "                \n",
        "                is_successful, frame = video_capture.read()\n",
        "                frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "                #Down sampled to 1 frame per 15.\n",
        "                if index % 15 == 0:\n",
        "                \n",
        "                    major_feature,minor_feature = self._extract_features(Image.fromarray(frame))\n",
        "                    '''\n",
        "                    print(\"MAJOR FEATURES\")\n",
        "                    print(major_feature)\n",
        "                    print(\"MINOR FEATURES\")\n",
        "                    print(minor_feature)\n",
        "                    '''\n",
        "                    major_features.append(major_feature)\n",
        "                    minor_features.append(minor_feature)\n",
        "                            \n",
        "            video_capture.release()\n",
        "            self._create_attributes(video_name, fps, number_of_frames, major_features,minor_features)\n",
        "        return\n",
        "\n",
        "\n",
        "    def get_dataset(self):\n",
        "        return self.dataset\n",
        "    "
      ],
      "metadata": {
        "id": "GgcilaOCg22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':  \n",
        "    save_path = f'summe_gnet_{int(time.time())}.h5'\n",
        "    #Dictorty Containing videos:\n",
        "    video_directory = \"/content/drive/MyDrive/Feature Extraction - Mabon/GoogLeNet_Features-P2\"\n",
        "    #Creating Object of the Features Exractor:\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.perform_extraction(save_path = save_path,\n",
        "                                         video_path = video_directory)\n",
        "    \n",
        "    dataset = feature_extractor.get_dataset()"
      ],
      "metadata": {
        "id": "3aIow2-giTWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e3880a-4646-435f-f2e8-409d74529cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [08:03<00:00, 483.82s/it]\n"
          ]
        }
      ]
    }
  ]
}